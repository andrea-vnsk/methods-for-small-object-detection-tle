{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a4d2bfa-9e9a-4552-af59-39c2551535e3",
   "metadata": {},
   "source": [
    "# **DEtection TRansformer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5094a8b-5bd8-420d-b069-88ee8e5c7aed",
   "metadata": {},
   "source": [
    "Postup vytvorenia modelu DETR v tomto notebooku je založený na <a href=\"https://github.com/roboflow/notebooks/blob/main/notebooks/train-huggingface-detr-on-custom-dataset.ipynb\">originálnom notebooku.</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3cff45-a7d1-4723-bea6-6a78515cd8a3",
   "metadata": {},
   "source": [
    "### Príprava prostredia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b973da74-21b3-4daa-8430-8171cc3b125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5738e37-555e-41b7-a75f-400498324d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install supervision==0.3.0\n",
    "!pip install -q transformers\n",
    "!pip install -q pytorch-lightning\n",
    "!pip install -q roboflow\n",
    "!pip install -q timm\n",
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026311bd-7ac2-4109-92c7-1d9bf0735e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "\n",
    "import roboflow\n",
    "import supervision\n",
    "import transformers\n",
    "import pytorch_lightning\n",
    "\n",
    "print(\n",
    "    #\"roboflow:\", roboflow.__version__,\n",
    "    \"; supervision:\", supervision.__version__,\n",
    "    \"; transformers:\", transformers.__version__,\n",
    "    \"; pytorch_lightning:\", pytorch_lightning.__version__\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0293073b-930d-4f79-9a0a-3b12159215ab",
   "metadata": {},
   "source": [
    "### Načítanie predtrénovaného modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898eb791-0a35-4543-af9a-525f13c4bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DetrForObjectDetection, DetrImageProcessor\n",
    "\n",
    "\n",
    "# settings\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "CHECKPOINT = 'facebook/detr-resnet-50'\n",
    "CONFIDENCE_TRESHOLD = 0.5\n",
    "IOU_TRESHOLD = 0.8\n",
    "num_classes = 1\n",
    "\n",
    "image_processor = DetrImageProcessor.from_pretrained(CHECKPOINT)\n",
    "model = DetrForObjectDetection.from_pretrained(CHECKPOINT, num_labels=num_classes, ignore_mismatched_sizes=True)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c728d3-8310-4080-9ec2-401d063cf5eb",
   "metadata": {},
   "source": [
    "### Príprava dát "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c739df08-bebb-468f-9b0f-ba23614a1e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision\n",
    "\n",
    "ANNOTATION_FILE_NAME = \"_annotations.coco.json\" # Názov COCO anotácií v súbore .json\n",
    "\n",
    "# Cesty jednotlivým obrázkom množín datasetu\n",
    "TRAIN_DIRECTORY = \"../dataset/train/images/\"\n",
    "VAL_DIRECTORY = \"../dataset/val/images/\"\n",
    "TEST_DIRECTORY = \"../dataset/TP_test/images\"\n",
    "\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_directory_path: str,\n",
    "        image_processor,\n",
    "        train: bool = True\n",
    "    ):\n",
    "        annotation_file_path = os.path.join(image_directory_path, ANNOTATION_FILE_NAME)\n",
    "        super(CocoDetection, self).__init__(image_directory_path, annotation_file_path)\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images, annotations = super(CocoDetection, self).__getitem__(idx)\n",
    "        image_id = self.ids[idx]\n",
    "        annotations = {'image_id': image_id, 'annotations': annotations}\n",
    "        encoding = self.image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze()\n",
    "        target = encoding[\"labels\"][0]\n",
    "\n",
    "        return pixel_values, target\n",
    "\n",
    "\n",
    "TRAIN_DATASET = CocoDetection(\n",
    "    image_directory_path=TRAIN_DIRECTORY,\n",
    "    image_processor=image_processor,\n",
    "    train=True)\n",
    "VAL_DATASET = CocoDetection(\n",
    "    image_directory_path=VAL_DIRECTORY,\n",
    "    image_processor=image_processor,\n",
    "    train=False)\n",
    "TEST_DATASET = CocoDetection(\n",
    "    image_directory_path=TEST_DIRECTORY,\n",
    "    image_processor=image_processor,\n",
    "    train=False)\n",
    "\n",
    "print(\"Number of training examples:\", len(TRAIN_DATASET))\n",
    "print(\"Number of validation examples:\", len(VAL_DATASET))\n",
    "print(\"Number of test examples:\", len(TEST_DATASET))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5fbdef-deb0-413c-b593-9fde5f38717f",
   "metadata": {},
   "source": [
    "### Nastavenie konfigurácií pre tréning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02eaa80-0665-40de-97bc-876808bc3939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # DETR authors employ various image sizes during training, making it not possible\n",
    "    # to directly batch together images. Hence they pad the images to the biggest\n",
    "    # resolution in a given batch, and create a corresponding binary pixel_mask\n",
    "    # which indicates which pixels are real/which are padding\n",
    "    pixel_values = [item[0] for item in batch]\n",
    "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[1] for item in batch]\n",
    "    return {\n",
    "        'pixel_values': encoding['pixel_values'],\n",
    "        'pixel_mask': encoding['pixel_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "TRAIN_DATALOADER = DataLoader(dataset=TRAIN_DATASET, collate_fn=collate_fn, batch_size=4, shuffle=True)\n",
    "VAL_DATALOADER = DataLoader(dataset=VAL_DATASET, collate_fn=collate_fn, batch_size=4)\n",
    "TEST_DATALOADER = DataLoader(dataset=TEST_DATASET, collate_fn=collate_fn, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91edcd8-9a72-4ec9-be3d-435e0aebd14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from transformers import DetrForObjectDetection\n",
    "import torch\n",
    "\n",
    "\n",
    "class Detr(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, lr, lr_backbone, weight_decay):\n",
    "        super().__init__()\n",
    "        self.model = DetrForObjectDetection.from_pretrained(\n",
    "            pretrained_model_name_or_path=CHECKPOINT,\n",
    "            num_labels=len(id2label),\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "\n",
    "        self.lr = lr\n",
    "        self.lr_backbone = lr_backbone\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, pixel_values, pixel_mask):\n",
    "        return self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        pixel_mask = batch[\"pixel_mask\"]\n",
    "        labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "        outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss_dict = outputs.loss_dict\n",
    "\n",
    "        return loss, loss_dict\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        # logs metrics for each training_step, and the average across the epoch\n",
    "        self.log(\"training_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "            self.log(\"train_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        self.log(\"validation/loss\", loss)\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log(\"validation_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # DETR authors decided to use different learning rate for backbone\n",
    "        # you can learn more about it here:\n",
    "        # - https://github.com/facebookresearch/detr/blob/3af9fa878e73b6894ce3596450a8d9b89d918ca9/main.py#L22-L23\n",
    "        # - https://github.com/facebookresearch/detr/blob/3af9fa878e73b6894ce3596450a8d9b89d918ca9/main.py#L131-L139\n",
    "        param_dicts = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "            {\n",
    "                \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "                \"lr\": self.lr_backbone,\n",
    "            },\n",
    "        ]\n",
    "        return torch.optim.AdamW(param_dicts, lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return TRAIN_DATALOADER\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return VAL_DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e423cb5b-e1e9-4fb3-81ba-e69a53f0f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\n",
    "\n",
    "batch = next(iter(TRAIN_DATALOADER))\n",
    "outputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faa4f14-1aa5-431a-9b7f-ee6dc5ac407c",
   "metadata": {},
   "source": [
    "### Trénovanie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2683cfe-e4fd-432a-9fe9-eb7f3f8dcec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "%cd {HOME}\n",
    "\n",
    "MAX_EPOCHS = 100          # Počet epoch pre trénovanie \n",
    "\n",
    "# technika predčasného ukončenia trénovania\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',   # monitorovaná metrika\n",
    "    patience=15,          # počet epoch bez zlepšenia metriky, predtým ako sa zastaví trénovanie\n",
    "    verbose=True,         # zobrazenie informácií pre monitoring a debugging\n",
    "    mode='min',           # 'min' pre minimalizáciu metriky, 'max' pre maximalizáciu\n",
    "    min_delta=0.0001      # minimálna zmena, aby sa kvalifikovalo ako zlepšenie\n",
    ")\n",
    "\n",
    "trainer = Trainer(devices=1, \n",
    "                  accelerator=\"gpu\", \n",
    "                  max_epochs=MAX_EPOCHS, \n",
    "                  gradient_clip_val=0.1, \n",
    "                  accumulate_grad_batches=8, \n",
    "                  log_every_n_steps=5,  \n",
    "                  callbacks=[early_stop_callback])\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4284877e-ebb9-4962-9558-d9880cca65d3",
   "metadata": {},
   "source": [
    "### Uloženie natrénovaného modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27cddf-773b-4f5e-a7d9-a1a37f5a3488",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d35ec7-4496-43cc-ac12-0d10f56eb2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = os.path.join(HOME, 'custom-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d261b719-cca0-42c2-8f1f-b3c326ef6d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.save_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3ea384-413b-4394-8dbd-4e91d3de8144",
   "metadata": {},
   "source": [
    "### Načítanie natrénovaného modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee533ea-617d-42d8-a810-f2d4df4d7bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DetrForObjectDetection.from_pretrained(MODEL_PATH)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456884ab-4401-4ba8-8a2d-eb056dae45e2",
   "metadata": {},
   "source": [
    "### Vizualizácia predikcií\n",
    "\n",
    "Vykonanie vizualizácie na všetky obrázky v testovacej množine a následné uloženie do súborov:\n",
    "\n",
    "*inferenced*:\n",
    ">TP - súbor pre obrázky s detegovanými objektmi\n",
    " \n",
    ">TN - súbor pre obrázky bez detegovaných objektov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3bf18d-e70b-4ce9-983c-aa6f2c914a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nastavanie hraníc pre predikcie\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "NMS_THRESHOLD = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332a9af2-9ce5-401e-935c-1602e295cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "input_folder = TEST_DIRECTORY\n",
    "output_folder = \"inferenced\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "categories = TEST_DATASET.coco.cats\n",
    "id2label = {k: v['name'] for k, v in categories.items()}\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "\n",
    "detection_folder = os.path.join(output_folder, \"TP\")\n",
    "no_detection_folder = os.path.join(output_folder, \"TN\")\n",
    "\n",
    "for folder in [detection_folder, no_detection_folder]:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "for image_filename in os.listdir(input_folder):\n",
    "  \n",
    "    if image_filename.lower().endswith(\".jpg\"):\n",
    "        image_path = os.path.join(input_folder, image_filename)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = image_processor(images=image, return_tensors='pt').to(DEVICE)\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            target_sizes = torch.tensor([image.shape[:2]]).to(DEVICE)\n",
    "            results = image_processor.post_process_object_detection(\n",
    "                outputs=outputs,\n",
    "                threshold=CONFIDENCE_THRESHOLD,\n",
    "                target_sizes=target_sizes\n",
    "            )[0]\n",
    "            \n",
    "        detections = sv.Detections.from_transformers(transformers_results=results)\n",
    "        \n",
    "        if detections:\n",
    "            detections = sv.Detections.from_transformers(transformers_results=results).with_nms(threshold=NMS_THRESHOLD)\n",
    "\n",
    "            labels = [f\"{id2label[class_id]} {confidence:.2f}\" for _, confidence, class_id, _ in detections]\n",
    "            frame = box_annotator.annotate(scene=image.copy(), detections=detections, labels=labels)\n",
    "\n",
    "            output_folder = detection_folder\n",
    "\n",
    "        else:\n",
    "            frame = image.copy()\n",
    "            output_folder = no_detection_folder\n",
    "\n",
    "        output_image_path = os.path.join(output_folder, image_filename)\n",
    "        cv2.imwrite(output_image_path, frame)\n",
    "        print(f\"Annotated image saved: {output_image_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
